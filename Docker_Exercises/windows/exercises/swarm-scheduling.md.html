<html>
<head>
    <title>swarm-scheduling.md</title>
    <link href='https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css' rel='stylesheet' integrity='sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u' crossorigin='anonymous'>
    <link href="../../app.css" rel="stylesheet" >
</head>
<body>
    <nav class="navbar navbar-default">
    <div class="container">
        <!-- Brand and toggle get grouped for better mobile display -->
        <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1" aria-expanded="false">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand" href="../../index.html"><img class="logo" src="https://www.docker.com/sites/all/themes/docker/assets/images/brand-full.svg" alt="Docker" title="Docker"/></a>
        </div>
    </div><!-- /.container-fluid -->
    </nav>
    <div class="container">
    <div class="row">
        <div class="content">
            <h1 id="swarm-scheduling">Swarm Scheduling</h1>
<p>By default, the Swarm scheduling algorithm tries to spread workload out roughly evenly across your Swarm. In many cases, we want to exert more nuanced control over what containers get scheduled where, in order to respect resource availability, hardware requirements, application high availability and other concerns. By the end of this exercise, you should be able to:</p>
<ul>
<li>Impose memory resource reservations and limitations</li>
<li>Schedule services in global or replicated mode, and define appropriate use cases for each</li>
<li>Schedule tasks on a subset of nodes via label constraints</li>
<li>Schedule topology-aware services</li>
</ul>
<h2 id="restricting-resource-consumption">Restricting Resource Consumption</h2>
<p>By default, containers can consume as much compute resources as they want. Overconsumption of memory can cause an entire node to fail; as such, we need to make sure we&#39;re scheduling only as many containers on a host as that host can realistically support, and preventing them from allocating too much memory.</p>
<ol>
<li><p>On your worker node <code>node-3</code> (<strong>not</strong> on a manager node please), open the Task Manager, either through the search bar or by typing <code>taskmgr</code> in the command prompt, and click on <strong>More Details</strong> to get a report of current compute resource consumption.</p>
</li>
<li><p>Still on that same node, spin up a container that will allocate as much memory as it can:</p>
<pre><code class="lang-powershell">PS: node-3 Administrator&gt; docker container run `
    training/winstress:ws19 pwsh.exe C:\saturate-mem.ps1
</code></pre>
<p>After a minute, the memory column in Task Manager will max out, and your node will become unresponsive. Use <code>CTRL+C</code> to detach from the container, and <code>docker container rm -f</code> to remove this container.</p>
</li>
<li><p>If we spun this container up as a service, it could take down our entire swarm. Prevent this from happening by imposing a memory limitation on the service; from <code>node-0</code>:</p>
<pre><code class="lang-powershell">PS: node-0 Administrator&gt; docker service create `
    --replicas 4 --limit-memory 4096M --name memcap `
    training/winstress:ws19 pwsh.exe C:\saturate-mem.ps1
</code></pre>
<p>Observe the memory consumption through Task Manager on any node hosting a task for this service; it&#39;ll spike, but be capped before it can completely take down the node.</p>
</li>
<li><p>Observe your container&#39;s memory consumption directly with <code>docker stats</code> on any node hosting one of these containers:</p>
<pre><code class="lang-powershell">docker stats

CONTAINER ID  NAME             CPU %   PRIV WORKING SET ...
09fd6cef8528  memcap.1.rko...  48.29%  2.065GiB         ...
</code></pre>
<p>The private working set memory is capped below whatever limit you set with the <code>--limit-memory</code> flag. Use <code>CTRL+C</code> to break out of the <code>docker stats</code> view when done.</p>
</li>
<li><p>Clean up by deleting your service:</p>
<pre><code class="lang-powershell">PS: node-0 Administrator&gt; docker service rm memcap
</code></pre>
</li>
<li><p>So far, we&#39;ve limited the amount of resources a container can consume once scheduled, but we still haven&#39;t prevented the scheduler from <em>overprovisioning</em> containers on a node; we would like to prevent Docker from scheduling more containers on a node than that node can support. Delete and recreate your service one more time, this time also imposing a scheduling reservation with <code>--reserve-memory</code>:</p>
<pre><code class="lang-powershell">PS: node-0 Administrator&gt; docker service create `
    --replicas 4 --limit-memory 2048M --name memcap `
    --reserve-memory 2048M `
    training/winstress:ws19 pwsh.exe C:\saturate-mem.ps1

PS: node-0 Administrator&gt; docker service ps memcap    
</code></pre>
<p>You should see your four tasks running happily.</p>
</li>
<li><p>Now scale up your service to more replicas than your current cluster can support:</p>
<pre><code class="lang-powershell">PS: node-0 Administrator&gt; docker service update memcap --replicas=10 --detach
PS: node-0 Administrator&gt; docker service ps memcap

ID       NAME       CURRENT STATE           ERROR                         
...    
z6t...   memcap.5   Pending 2 seconds ago   &quot;no suitable node (insufficien…&quot; 
xf3...   memcap.6   Pending 2 seconds ago   &quot;no suitable node (insufficien…&quot;
...
</code></pre>
<p>Many of your tasks should be stuck in <code>CURRENT STATE: Pending</code>. Inspect one of them:</p>
<pre><code class="lang-powershell">PS: node-0 Administrator&gt; docker inspect &lt;task ID&gt;

...
    &quot;Status&quot;: {
        &quot;Timestamp&quot;: &quot;2019-01-29T16:31:45.5859483Z&quot;,
        &quot;State&quot;: &quot;pending&quot;,
        &quot;Message&quot;: &quot;pending task scheduling&quot;,
        &quot;Err&quot;: &quot;no suitable node (insufficient resources on 4 nodes)&quot;,
        &quot;PortStatus&quot;: {}
    }
...
</code></pre>
<p>From the <code>Status</code> block in the task info, we can see that this task isn&#39;t getting scheduled because there isn&#39;t sufficient resources available in the cluster to support it.</p>
<blockquote>
<p><strong>Always limit resource consumption</strong> using <em>both</em> limits and reservations. Failing to do so is a very common mistake that can lead to a widespread cluster outage; if your cluster is running at near-capacity and one node fails, its workload will get rescheduled to other nodes, potentially causing them to also fail, initiating a cascading cluster outage.</p>
</blockquote>
</li>
<li><p>Clean up by removing your service:</p>
<pre><code class="lang-powershell">PS: node-0 Administrator&gt; docker service rm memcap
</code></pre>
</li>
</ol>
<h2 id="configuring-global-scheduling">Configuring Global Scheduling</h2>
<p>So far, all the services we&#39;ve created have been run in the default <em>replicated mode</em>; as we&#39;ve seen, Swarm spreads containers out across the cluster, potentially respecting resource reservations. Sometimes, we want to run services that create <em>exactly one</em> container on each host in our cluster; this is typically used for deploying daemon, like logging, monitoring, or node management tools which need to run locally on every node.</p>
<ol>
<li><p>Create a globally scheduled service:</p>
<pre><code class="lang-powershell">PS: node-0 Administrator&gt; docker service create --mode global `
    --name my-global `
    mcr.microsoft.com/windows/nanoserver:10.0.17763.737 ping 8.8.8.8 -t
</code></pre>
</li>
<li><p>Check what nodes your service tasks were scheduled on:</p>
<pre><code class="lang-powershell">PS: node-0 Administrator&gt; docker service ps my-global

ID            NAME                      IMAGE                      NODE   
rjl91n1i5o4g  competent_taussig.j9h...  nanoserver:10.0.17763.737  node-3
pzkiv2kpsu26  competent_taussig.c3v...  nanoserver:10.0.17763.737  node-2
k767q7i1f73t  competent_taussig.afo...  nanoserver:10.0.17763.737  node-1
wem26fmzq2k5  competent_taussig.lnk...  nanoserver:10.0.17763.737  node-0
</code></pre>
<p>One task is scheduled on every node in the swarm; as you add or remove nodes from the swarm, the global service will be rescaled appropriately.</p>
</li>
<li><p>Remove your service with <code>docker service rm my-global</code>.</p>
</li>
</ol>
<h2 id="scheduling-via-node-constraints">Scheduling via Node Constraints</h2>
<p>Sometimes, we want to confine our containers to specific nodes; for this, we can use <em>constraints</em> and <em>node properties</em>.</p>
<ol>
<li><p>Add a label <code>datacenter</code> with value <code>east</code> to two nodes of your swarm:</p>
<pre><code class="lang-powershell">PS: node-0 Administrator&gt; docker node update --label-add datacenter=east node-0
PS: node-0 Administrator&gt; docker node update --label-add datacenter=east node-1
</code></pre>
</li>
<li><p>Add a label <code>datacenter</code> with value <code>west</code> to the other two nodes:</p>
<pre><code class="lang-powershell">PS: node-0 Administrator&gt; docker node update --label-add datacenter=west node-2
PS: node-0 Administrator&gt; docker node update --label-add datacenter=west node-3
</code></pre>
<p>Note these labels are user-defined; <code>datacenter</code> and its values <code>east</code> and <code>west</code> can be anything you like.</p>
</li>
<li><p>Schedule a service constrained to run on nodes labeled as <code>datacenter=east</code>:</p>
<pre><code class="lang-powershell">PS: node-0 Administrator&gt; docker service create --replicas 4 `
    --constraint node.labels.datacenter==east `
    --name east-deploy `
    mcr.microsoft.com/windows/nanoserver:10.0.17763.737 ping 8.8.8.8 -t
</code></pre>
</li>
<li><p>Check what nodes your tasks were scheduled on as above; they should all be on nodes bearing the <code>datacenter==east</code> label (<code>node-0</code> and <code>node-1</code>).</p>
</li>
<li><p>Remove your service, and schedule another, this time constrained to run only on worker nodes:</p>
<pre><code class="lang-powershell">PS: node-0 Administrator&gt; docker service rm east-deploy
PS: node-0 Administrator&gt; docker service create --replicas 4 `
    --constraint node.role==worker `
    --name worker-only `
    mcr.microsoft.com/windows/nanoserver:10.0.17763.737 ping 8.8.8.8 -t
</code></pre>
<p>Once again, check where your <code>worker-only</code> service tasks got scheduled; they&#39;ll all be on <code>node-3</code>, your only worker.</p>
<blockquote>
<p><strong>Keep workload off of managers</strong> using these selectors, especially in production. If something goes badly wrong with a workload container and causes its host to crash, we don&#39;t want to take down a manager and possibly lose our raft consensus. As we&#39;ve seen, Swarm can recover from losing workers automatically; a lost manager consensus can be much harder to recover from.</p>
</blockquote>
</li>
<li><p>Clean up by removing this service: <code>docker service rm worker-only</code>.</p>
</li>
</ol>
<h2 id="scheduling-topology-aware-services">Scheduling Topology-Aware Services</h2>
<p>Oftentimes, we want to schedule workload to be tolerant of faults in our datacenters; we wouldn&#39;t want every replica for a service on one power zone or one rack which can go down all at once, for example. </p>
<ol>
<li><p>Create a service using the <code>--placement-pref</code> flag to spread replicas across our <code>datacenter</code> label:</p>
<pre><code class="lang-powershell">PS: node-0 Administrator&gt; docker service create --name spreaddemo `
    --replicas=2 --publish 8000:80 `
    --placement-pref spread=node.labels.datacenter `
    mcr.microsoft.com/windows/nanoserver:10.0.17763.737 ping 8.8.8.8 -t
</code></pre>
<p>There should be <code>nanoserver</code> containers present on nodes with every possible value of the <code>node.labels.datacenter</code> label, one in <code>datacenter=east</code> nodes, and one in <code>datacenter=west</code> nodes.</p>
</li>
<li><p>Use <code>docker service ps spreaddemo</code> as above to check that replicas got spread across the datacenter labels.</p>
</li>
<li><p>Clean up: <code>docker service rm spreaddemo</code>.</p>
</li>
</ol>
<h2 id="conclusion">Conclusion</h2>
<p>In this exercise, we saw how to use resource allocations, global scheduling, labels and node properties to influence scheduling. A few best practices:</p>
<ul>
<li><strong>Always apply memory and CPU limits and reservations</strong>, and in essentially all cases, the limit should be less than or equal to the reservation to ensure nodes are never overprovisioned.</li>
<li><strong>Keep workload off of manager</strong> as mentioned above, to prevent application logic bugs from taking down your manager consensus</li>
<li><strong>Don&#39;t overconstrain your scheduler</strong>: it can be tempting to exert strict control over exactly what gets scheduled exactly where; don&#39;t. If a service is constrained to run on a very small set of nodes, and those nodes go down, the service will become unschedulable and suffer an outage. Let your orchestraor&#39;s scheduler make decisions as independently as possible in order to maximize workload resilience.</li>
</ul>

        </div>        
    </div>
    <div class="row">
        <ul class="mt-article-pagination" style="display:block;">
        </ul>
    </div>
</div>
    <div class="footer"></div>
</body>